–î–ª—è –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –æ–±–º–µ–∂–µ–Ω–∏—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó–≤ —Ç—Ä–µ–±–∞ –¥–æ–¥–∞—Ç–∏ token –¥–ª—è —Å–∞–π—Ç—É huggingface.co, –¥–ª—è —á–æ–≥–æ –∑–º—ñ–Ω–∏—Ç–∏ —Ñ–∞–π–ª `./scripts/setup`:<br />
–∑–º—ñ–Ω–∏—Ç–∏ —Ä—è–¥–æ–∫: "from huggingface_hub import hf_hub_download, snapshot_download",<br />
–¥–æ–¥–∞—Ç–∏ —ñ–º–ø–æ—Ä—Ç "login", —â–æ–± –≤–∏–π—à–ª–æ —Ç–∞–∫–∏–º —á–∏–Ω–æ–º:<br />
"from huggingface_hub import hf_hub_download, snapshot_download, login"<br />
–ü–æ—Ç—ñ–º, –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ "if __name__ == '__main__':" –¥–æ–¥–∞—Ç–∏ —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—é –Ω–∞ —Å–∞–π—Ç—ñ:<br />
"login(token='my_token')", –¥–µ 'my_token' –∑–º—ñ–Ω–∏—Ç–∏ –Ω–∞ —Å–≤—ñ–π —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø—É –Ω–∞ —Å–∞–π—Ç https://huggingface.co<br /><br />
–î–ª—è –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –±—É—Ñ–µ—Ä—É –ø–∞–º'—è—Ç—ñ –¥–ª—è —á–∞—Ç-–¥–≤–∏–≥—É–Ω–∞ —Ç—Ä–µ–±–∞ –∑–º—ñ–Ω–∏—Ç–∏ —Ñ–∞–π–ª `./private_gpt/server/chat/chat_service.py`:

    from llama_index.core.memory import ChatMemoryBuffer   # üî¥red=31<-- ADD IMPORT;

    def _chat_engine(
        self,
        system_prompt: str | None = None,
        use_context: bool = False,
        context_filter: ContextFilter | None = None,
    ) -> BaseChatEngine:
        if use_context:
            vector_index_retriever = self.vector_store_component.get_retriever(
                index=self.index, context_filter=context_filter
            )
            memory = ChatMemoryBuffer.from_defaults(token_limit=8192)  # <-- MORE TOKENS
            return ContextChatEngine.from_defaults(
                system_prompt=system_prompt,
                retriever=vector_index_retriever,
                memory=memory,  #  <-- USE THE  LARGER BUFFER
                llm=self.llm_component.llm,
                node_postprocessors=[
                    MetadataReplacementPostProcessor(target_metadata_key="window"),
                ],
            )
